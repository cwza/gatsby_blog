---
title: 'AlphaGo and AlphaZero'
date: 2020-06-08
tags: ['reinforcement-learning', 'deep-learning']
description: 'AlphaGo and AlphaZero notes'
draft: false
hide: false
---

## AlphaGo
* Value Network: state |-> predicted score
* Policy Network: state |-> the best next step
* Monte-Carlo Tree Search: Traditional tree search method to find the best step
* Key: Use policy network to reduce the search breadth, and use value network to reduce the search depth

1. 使用過去棋譜初始化policy network與value network(模擬人類高手下棋)
2. 讓AI使用train好的network自我對弈產生更多高手棋譜
3. 利用這些資料來train一個更好的policy network與value network
4. Loop 2, 3, 來產生一個非常好的policy network與value network
5. 在實際對弈中使用Monte-Carlo tree search並結合pretrain好的policy network與value network來降低search的深度與廣度

## AlphaZero

1. 隨機初始化一個policy network與value network
2. 讓AI自我對弈, 使用Monte-Carlo tree search結合policy network與value netowrk來產生資料
3. 利用這些資料來train一個更好的policy network與value network
4. Loop 2, 3, 來產生一個非常好的policy network與value network
5. 在實際對弈中使用Monte-Carlo tree search並結合pretrain好的policy network與value network來降低search的深度與廣度

AlphaGo與AlphaZero最大的不同在:
1. AlphaZero不再使用人類的棋譜來初始化, 而是隨機初始化
2. AlphaZero將Monte-Carlo tree search放入training process, 而AlphaGo只將Monte-Carlo search放在最後inference時

## Reference
[David Silver - Deep Reinforcement Learning from AlphaGo to AlphaStar](https://www.youtube.com/watch?v=x5Q79XCxMVc&t=5s)